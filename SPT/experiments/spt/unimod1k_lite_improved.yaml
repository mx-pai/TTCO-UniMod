DATA:
  MAX_SAMPLE_INTERVAL: 200
  MAX_SEQ_LENGTH: 40
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  SEARCH:
    CENTER_JITTER: 5.5  # UPGRADED from 4.5
    FACTOR: 5.0
    SCALE_JITTER: 0.6  # UPGRADED from 0.5
    SIZE: 320
  STD:
  - 0.229
  - 0.224
  - 0.225
  TEMPLATE:
    CENTER_JITTER: 1.0  # UPGRADED from 0
    FACTOR: 2.0
    SCALE_JITTER: 0.1  # UPGRADED from 0
    SIZE: 128
  TRAIN:
    DATASETS_NAME:
      - UniMod1K
    DATASETS_RATIO:
      - 1
    SAMPLE_PER_EPOCH: 30000

MODEL:
  PRETRAINED: '/root/autodl-tmp/STARKS_ep0500.pth.tar'
  INIT: 'xavier'
  BACKBONE:
    DILATION: false
    OUTPUT_LAYERS:
    - layer3
    TYPE: resnet50
  HEAD_TYPE: CORNER
  HIDDEN_DIM: 256
  NUM_OBJECT_QUERIES: 1
  POSITION_EMBEDDING: sine
  PREDICT_MASK: false
  TRANSFORMER:
    DEC_LAYERS: 6
    DIM_FEEDFORWARD: 2048
    DIVIDE_NORM: false
    DROPOUT: 0.1
    ENC_LAYERS: 6
    NHEADS: 8
    PRE_NORM: false
  LANGUAGE:
    IMPLEMENT: 'pytorch'
    TYPE: 'bert-base-uncased'
    PATH: '/root/autodl-tmp/bert/bert-base-uncased.tar.gz'
    VOCAB_PATH: '/root/autodl-tmp/bert/bert-base-uncased-vocab.txt'
    BERT:
      LR: 0.0001
      ENC_NUM: 12
      HIDDEN_DIM: 256
      MAX_QUERY_LEN: 40

TRAIN:
  BACKBONE_MULTIPLIER: 0.15  # UPGRADED from 0.1
  BATCH_SIZE: 16
  DEEP_SUPERVISION: false
  EPOCH: 240
  FREEZE_BACKBONE_BN: true
  FREEZE_LAYERS:
  - conv1
  - layer1
  GIOU_WEIGHT: 2.5  # UPGRADED from 2.0
  GRAD_CLIP_NORM: 0.1
  L1_WEIGHT: 4.0  # ADJUSTED from 5.0
  LR: 0.00002  # UPGRADED from 0.00001
  LR_DROP_EPOCH: 240
  NUM_WORKER: 8
  OPTIMIZER: ADAMW
  PRINT_INTERVAL: 50
  SCHEDULER:
    TYPE: Mstep  # UPGRADED from step
    MILESTONES: [80, 120, 160, 200]
    GAMMA: 0.1
  VAL_EPOCH_INTERVAL: 20
  WEIGHT_DECAY: 0.0002  # UPGRADED from 0.0001

TEST:
  EPOCH: 240
  SEARCH_FACTOR: 5.0
  SEARCH_SIZE: 320
  TEMPLATE_FACTOR: 2.0
  TEMPLATE_SIZE: 128

