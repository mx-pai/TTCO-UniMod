DATA:
  MAX_SAMPLE_INTERVAL: 200
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  SEARCH:
    CENTER_JITTER: 5.5
    FACTOR: 5.0
    SCALE_JITTER: 0.6
    SIZE: 320
  STD:
  - 0.229
  - 0.224
  - 0.225
  TEMPLATE:
    CENTER_JITTER: 1.0
    FACTOR: 2.0
    SCALE_JITTER: 0.1
    SIZE: 128
  TRAIN:
    DATASETS_NAME:
      - UniMod1K
    DATASETS_RATIO:
      - 1
    SAMPLE_PER_EPOCH: 30000
    LONG_SEQ_RATIO: 0.3
    LONG_SEQ_LENGTH: 4

MODEL:
  PRETRAINED: '/root/autodl-tmp/STARKS_ep0500.pth.tar'
  BACKBONE:
    DILATION: false
    OUTPUT_LAYERS:
    - layer3
    TYPE: resnet50
  HEAD_TYPE: CORNER
  HIDDEN_DIM: 256
  NUM_OBJECT_QUERIES: 1
  POSITION_EMBEDDING: sine
  PREDICT_MASK: false
  TRANSFORMER:
    DEC_LAYERS: 6
    DIM_FEEDFORWARD: 2048
    DIVIDE_NORM: false
    DROPOUT: 0.1
    ENC_LAYERS: 6
    NHEADS: 8
    PRE_NORM: false
  LANGUAGE:
    IMPLEMENT: 'pytorch'
    TYPE: 'bert-base-uncased'
    PATH: '/root/autodl-tmp/bert/bert-base-uncased.tar.gz'
    VOCAB_PATH: '/root/autodl-tmp/bert/bert-base-uncased-vocab.txt'
TRAIN:
  BACKBONE_MULTIPLIER: 0.15
  BATCH_SIZE: 16
  DEEP_SUPERVISION: false
  EPOCH: 240
  FREEZE_BACKBONE_BN: true
  FREEZE_LAYERS:
  - conv1
  - layer1
  GIOU_WEIGHT: 2.5
  GRAD_CLIP_NORM: 0.1
  L1_WEIGHT: 4.0
  LR: 0.00002
  LR_DROP_EPOCH: 240
  NUM_WORKER: 8
  OPTIMIZER: ADAMW
  PRINT_INTERVAL: 50
  SCHEDULER:
    TYPE: Mstep
    DECAY_RATE: 0.5
    MILESTONES: [80, 120, 160, 200]
    GAMMA: 0.5
  VAL_EPOCH_INTERVAL: 20
  WEIGHT_DECAY: 0.0002
  LANG_WEIGHT: 1.0
TEST:
  EPOCH: 240
  SEARCH_FACTOR: 5.0
  SEARCH_SIZE: 320
  TEMPLATE_FACTOR: 2.0
  TEMPLATE_SIZE: 128
  LANG_THRESHOLD: 0.3
PATHS:
  OUTPUT_DIR: '/root/autodl-tmp/spt_runs'
  PRETRAINED_DIR: '/root/autodl-tmp'
  DATA_ROOT: '/root/autodl-tmp/data/1-训练验证集/TrainSet'
  NLP_ROOT: '/root/autodl-tmp/data/1-训练验证集/TrainSet'
